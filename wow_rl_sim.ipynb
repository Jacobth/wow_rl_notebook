{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from keras.initializers import RandomUniform\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bridge_start_west = np.array([-3727.41, 1668.2])\n",
    "bridge_start_east = np.array([-3735.6, 1668.89])\n",
    "\n",
    "bridge_end_west = np.array([-3734.51, 1579.9])\n",
    "bridge_end_east = np.array([-3742.36, 1580.42])\n",
    "\n",
    "end_avg = np.sum([bridge_end_west, bridge_end_east], axis=0) / 2\n",
    "\n",
    "bridge_b = np.abs(bridge_end_east[1] - bridge_start_east[1])\n",
    "bridge_a = np.abs(bridge_end_east[0] - bridge_start_east[0])\n",
    "bridge_angle = np.arctan(bridge_a / bridge_b)\n",
    "\n",
    "start_angle = bridge_angle\n",
    "\n",
    "init_pos = np.sum([bridge_start_west, bridge_start_east], axis=0) / 2\n",
    "\n",
    "k = (bridge_end_west[1] - bridge_end_east[1]) / (bridge_end_west[0] - bridge_end_east[0])\n",
    "m = bridge_end_west[1] -(k * bridge_end_west[0])\n",
    "\n",
    "step_size = np.abs((bridge_start_west[0] - bridge_start_east[0]) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_goal(x, y):\n",
    "      \n",
    "    goal_y = (k*x) + m\n",
    "    \n",
    "    return y < goal_y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_within_bridge(x, y):\n",
    "    \n",
    "    point = Point(x, y)\n",
    "    polygon = Polygon([bridge_start_west, bridge_start_east, bridge_end_east, bridge_end_west])\n",
    "    \n",
    "    return polygon.contains(point) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_right_edge():\n",
    "    \n",
    "    k_r = (bridge_start_west[1] - bridge_end_west[1]) / (bridge_start_west[0] - bridge_end_west[0])\n",
    "    m_r = bridge_end_west[1] -(k_r * bridge_end_west[0])\n",
    "    \n",
    "    return k_r, m_r\n",
    "    \n",
    "k_r, m_r = get_right_edge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_dist(x, y):\n",
    "    \n",
    "    k_p = -1 / k_r\n",
    "    m_p = y - (k_p * x);\n",
    "\n",
    "    x_new = (m_p - m_r) / (k_r - k_p);\n",
    "    y_new = (k_r*x_new) + m_r;\n",
    "\n",
    "    a = (x_new - x);\n",
    "    b = (y_new - y);\n",
    "\n",
    "    d = np.sqrt(a**2 + b**2);\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_angle(action):  \n",
    "    \n",
    "    return{\n",
    "        0 : bridge_angle,\n",
    "        1 : bridge_angle - (np.pi / 4),\n",
    "        2 : bridge_angle + (np.pi / 4),\n",
    "    }[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_action(action, x, y):\n",
    "        \n",
    "    v = get_new_angle(action)\n",
    "    \n",
    "    a = step_size * np.cos(v);\n",
    "    b = step_size * np.sin(v);\n",
    "\n",
    "    x_new = x - b;\n",
    "    y_new = y - a;\n",
    "    \n",
    "    return x_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminate_reward = 1000\n",
    "step_reward = -0.01\n",
    "stuck_reward = -1000\n",
    "\n",
    "state_size = 3\n",
    "action_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(action, p_x, p_y):\n",
    "    \n",
    "    tmp_player_x = p_x\n",
    "    tmp_player_y = p_y\n",
    "    \n",
    "    new_pos = do_action(action, p_x, p_y)\n",
    "    \n",
    "    p_x = new_pos[0]\n",
    "    p_y = new_pos[1]\n",
    "    \n",
    "    reward = 0\n",
    "    done = False\n",
    "    \n",
    "    if is_goal(p_x, p_y):\n",
    "        done = True\n",
    "        reward = terminate_reward\n",
    "    \n",
    "    elif is_within_bridge(p_x, p_y):\n",
    "        \n",
    "        #stuck = is_stuck(p_x, p_y)\n",
    "        #dist = get_distance(p_x, p_y)\n",
    "        \n",
    "        reward = step_reward #* (dist / 100)\n",
    "        \n",
    "        #if stuck:\n",
    "         #   reward = stuck_reward\n",
    "          #  p_x = tmp_player_x\n",
    "          #  p_y = tmp_player_y\n",
    "            #done = True\n",
    "            \n",
    "    else:\n",
    "        p_x = tmp_player_x\n",
    "        p_y = tmp_player_y\n",
    "        reward = stuck_reward;\n",
    "        #done = True\n",
    "            \n",
    "    state = np.reshape([p_x, p_y, get_edge_dist(p_x, p_y)], [1, state_size])\n",
    "        \n",
    "    return state, reward, done, p_x, p_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset():\n",
    "    \n",
    "    p_x = init_pos[0]\n",
    "    p_y = init_pos[1]\n",
    "    \n",
    "    state = np.reshape([p_x, p_y, get_edge_dist(p_x, p_y)], [1, state_size])\n",
    "    \n",
    "    return state, p_x, p_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, discount, eps, eps_decay, eps_min, l_rate, decay_linear):\n",
    "        self.path = \"my_model.h5\"\n",
    "        self.state_size = state_size       \n",
    "        self.action_size = action_size\n",
    "        self.mem = deque(maxlen=2000)\n",
    "        self.discount = discount\n",
    "        self.eps = eps\n",
    "        self.eps_decay = eps_decay\n",
    "        self.decay_linear = decay_linear\n",
    "        self.eps_min = eps_min\n",
    "        self.l_rate = l_rate\n",
    "        self.model = self.init_model()\n",
    "\n",
    "    def save_model(self):\n",
    "        self.model.save(self.path)\n",
    "\n",
    "    def init_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(30, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.l_rate))\n",
    "\n",
    "        return model\n",
    "\n",
    "    def action(self, state):\n",
    "\n",
    "        if np.random.rand() <= self.eps:\n",
    "            return random.randrange(self.action_size)   \n",
    "        \n",
    "        actions = self.model.predict(state)\n",
    "        \n",
    "        return np.argmax(actions[0])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, terminal):\n",
    "        self.mem.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \n",
    "        if len(self.mem) < batch_size:\n",
    "            batch = self.mem\n",
    "        else:\n",
    "            batch = random.sample(self.mem, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, terminal in batch:\n",
    "            target = reward\n",
    "\n",
    "            if not terminal:\n",
    "                target += self.discount * np.amax(self.model.predict(next_state)[0])\n",
    "            \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "        \n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        \n",
    "        self.decay()\n",
    "\n",
    "\n",
    "    def decay(self):\n",
    "        if self.eps > self.eps_min:\n",
    "            if self.decay_linear:\n",
    "                self.eps -= self.eps_decay\n",
    "            else:\n",
    "                self.eps *= self.eps_decay\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1000 done, found goal 1000 times with and avg step of 33.401, total goals: 1000\n",
      "episode 2000 done, found goal 1000 times with and avg step of 32.761, total goals: 2000\n",
      "episode 3000 done, found goal 1000 times with and avg step of 32.019, total goals: 3000\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(\n",
    "            state_size = state_size, \n",
    "            action_size = action_size, \n",
    "            discount = 0.98, \n",
    "            eps = 1, \n",
    "            eps_decay = 1 / 25000, \n",
    "            eps_min = 0.001, \n",
    "            l_rate = 0.0001,\n",
    "            decay_linear = True\n",
    "            )\n",
    "\n",
    "#Train agent\n",
    "episodes = 25000\n",
    "steps = 200\n",
    "\n",
    "goalCounter = 0\n",
    "goalAvg = -1\n",
    "stepCounter = 0\n",
    "epGoalCounter = 0\n",
    "epGoalAvg = -1\n",
    "epStepCounter = 0\n",
    "\n",
    "for ep in range(1, episodes):\n",
    "    \n",
    "    state, p_x, p_y = reset()\n",
    "\n",
    "    player_x = p_x\n",
    "    player_y = p_y\n",
    "    \n",
    "    for st in range(1, steps):\n",
    "        \n",
    "        action = agent.action(state)\n",
    "\n",
    "        next_state, reward, done, p_x, p_y = step(action, player_x, player_y)\n",
    "        \n",
    "        player_x = p_x\n",
    "        player_y = p_y\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            goalCounter += 1\n",
    "            epGoalCounter += 1\n",
    "            epStepCounter += st\n",
    "            epGoalAvg = epStepCounter / epGoalCounter \n",
    "            break\n",
    "       \n",
    "    agent.replay(64)\n",
    "    \n",
    "    if ep % 1000 == 0:\n",
    "        agent.save_model()\n",
    "        print(\"episode {} done, found goal {} times with and avg step of {}, total goals: {}\".format(ep, epGoalCounter, epGoalAvg, goalCounter))\n",
    "        epGoalCounter = 0\n",
    "        epGoalAvg = -1\n",
    "        epStepCounter = 0\n",
    "\n",
    "\n",
    "agent.save_model()\n",
    "print(\"summary: total goals found: {}, an avg of {} episodes reached the goal\".format(goalCounter, goalCounter/episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
